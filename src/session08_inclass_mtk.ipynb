{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Language modelling with RNNs (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # make all text lowercase, keep strings that arent punctuation\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # utf8 encoding makes up for accents\n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, \n",
    "                        10, \n",
    "                        input_length=input_len))\n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1)) # remove 10% of the weight when the model is training\n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], \n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list),\n",
    "                                            axis=1)   \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"../in/news_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to load the data one at a time and append *only* the headlines to our list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Comments' in filename:\n",
    "        comment_df = pd.read_csv(data_dir + \"/\" + filename)\n",
    "        all_comments.extend(list(comment_df[\"commentBody\"].values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean up a little bit and see how many data points we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418481"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments = [c for c in all_comments if c != \"Unknown\"] # remove all unknown headlines\n",
    "len(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sample from all_comments\n",
    "import random\n",
    "sample_comments = random.sample(all_comments, 10)\n",
    "len(sample_comments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call out ```clean_text()``` function and then inspect the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['guess i dont get the joke here  ',\n",
       " 'fuggetabout the opioid crisis brbrmeanwhile let anyone purchase several semiautomatic guns and or a bumpstock kit at will and carry them across state lines or purchase alcohol at any time of day for legal consumption brbrbut get stopped for a broken tail light and have one cannabis cigarette in possession and go to jail ',\n",
       " 'the diacritical marks in mtley cre arent umlauts but rather dieresesbrbrwhatever they are called they are simply gratitous',\n",
       " 'vote for candidates who publicly and loudly denounce the nra and its blood money they use to bribe spineless politicians  vote for candidates who have never accepted money from the nra  vote for candidates who pledge never to touch one thin dime of nra moneybrvote against any candidate who has ever accepted nra money with the exception of a politician who gave back the money and publicly and loudly denounces the nra',\n",
       " 'how about once we stop focusing on the source of these cyber attacks we make a concerted effort to innoculate against future attacks by simply educating ourselves particularly our children against false claims and emotional attempts to sway our opinions  these disruptions would have no effect if we just ignored them or even better just turned off our obsessions with social media from time to time  these cyber attacks would have no effect if we werent stupid enough to believe them',\n",
       " '36a i realize many people treat till like an abbreviation of until and so spell it til but it isntbrbra hrefhttpwwwoedcomviewentry202001rskeym6crcpampresult11eid titlehttpwwwoedcomviewentry202001rskeym6crcpampresult11eid targetblankhttpwwwoedcomviewentry202001rskeym6crcpampampresult11eida',\n",
       " 'schumer is just recognizing trump for the con artist that he is  oldest trick in the book trump now knows how much schumer the patsy will put on the table his angle will be to offer very little in return with the sting that schumers offer is the new starting point',\n",
       " 'the problem in general with the republican agenda eg gun control criminal justice immigration climate change public health welfare etc is that it is based on platitudes  platitudes are nice to say and can sometimes be supported with anecdotes and make great taking points in a campaign  however policy needs to based on data and outcomes as long as one party is rooted in platitudes gop and the other in data dem and the gop base allows this we can never have a meaningful dialogue about anything brbrfor instance the debate on firearms should be framed as the cost of our current view of the 2nd amendment and mostly unrestricted firearm ownership is xx deaths per year is relatively unrestricted right of firearm owners worth that cost   brbri think the gop and its corporate supporters know how such factbased debates would turn out which is why they prefer to remain rooted in platitudes and anecdotes',\n",
       " 'i agree this is a sad situation  prior congresses and presidents gave the salvadorans the false expectation that they would remain in the us in perpetuity  on the other hand the salvadorans knew that at some future time their temporary status might require their exit they played the system and rolled the dice and they knew it  what troubles me is that we have invested in the expensive education of their children provided medical care perhaps food stamps and assistant grants  this article makes no mention of how many of the 200000 sought citizenship and its not encouraging that the one salvadoran they chose to interview couldnt speak english  i am further disturbed that we have sent over 45 b in foreign aid to salvador and it still remains one of the most dangerous places on the face of the earth  add to that salvadors export of gangs and the fact that the tpsers contribute to the over 60byear remittances flowing south and i have serious questions  if the salvadorans return to el salvador they do so better educated with a yearning for the freedom we have provided them  perhaps they can do in their homeland what our billions havent  turn el salvador into a functioning democracy  ',\n",
       " 'any democracy would certainly rely on an informed populace sadly that is a rare find in these days']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in sample_comments]\n",
    "corpus[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We're then going to tokenize our data, using the ```Tokenizer()``` class from ```TensorFlow```, about which you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "We then use the ```get_sequence_of_tokens()``` function we defined above, which turns every text into a sequence of tokens based on the vocabulary from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code creates an index of the words in the corpus\n",
    "tokenizer = Tokenizer()\n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1 # +1 accounts for words that arent in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[92, 15],\n",
       " [92, 15, 93],\n",
       " [92, 15, 93, 50],\n",
       " [92, 15, 93, 50, 1],\n",
       " [92, 15, 93, 50, 1, 94],\n",
       " [92, 15, 93, 50, 1, 94, 95],\n",
       " [96, 1],\n",
       " [96, 1, 97],\n",
       " [96, 1, 97, 98],\n",
       " [96, 1, 97, 98, 99]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take input and turn it into numerical output\n",
    "inp_sequences = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10]\n",
    "# the length of the input sequences are defined by how the words relate to each other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to *pad* our input sequences to make them all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding makes different inputs the same length as the longest input by adding 0's where words are missing in inputs\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n",
    "# E.g. with \"My cat\" and \"The big dog\" a 0 would be added in the front like \"0 My cat\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the ```create_model()``` function created above to initialize a model, telling the model the length of sequences and the total size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 206, 10)           3740      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 374)               37774     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,914\n",
      "Trainable params: 85,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 15:32:46.019399: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-28 15:32:46.022604: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-28 15:32:46.025451: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is exactly the same as last week, but instead of document labels, we're fitting the model to predict next word.\n",
    "\n",
    "*NB!* This will take some time to train! It took me 35 minutes on UCloud 32xCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 3s 461ms/step - loss: 3.4714\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 3s 472ms/step - loss: 3.4583\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 3s 431ms/step - loss: 3.3930\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 3.3549\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 3.3275\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 3.2753\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 3s 474ms/step - loss: 3.2526\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 3.2086\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 3s 381ms/step - loss: 3.1697\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 3s 567ms/step - loss: 3.1361\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 3.0983\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 3.0581\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 2s 281ms/step - loss: 3.0328\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 3s 424ms/step - loss: 2.9842\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 2s 355ms/step - loss: 2.9610\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 2.9197\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 2s 346ms/step - loss: 2.8968\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 2s 375ms/step - loss: 2.8572\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 2.8138\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 2s 295ms/step - loss: 2.7700\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 2s 288ms/step - loss: 2.7220\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 2.7205\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 2s 295ms/step - loss: 2.6807\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 2s 275ms/step - loss: 2.6411\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 2s 264ms/step - loss: 2.6241\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 2s 275ms/step - loss: 2.5827\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 2.5362\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 2.4973\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 2.4971\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 2s 348ms/step - loss: 2.4441\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 2s 297ms/step - loss: 2.4285\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 2.3980\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 2s 283ms/step - loss: 2.3649\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 2s 285ms/step - loss: 2.3255\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 2s 306ms/step - loss: 2.2974\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 2s 263ms/step - loss: 2.2774\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 2s 292ms/step - loss: 2.2600\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 2s 367ms/step - loss: 2.2022\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 2s 292ms/step - loss: 2.1686\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 2s 262ms/step - loss: 2.1444\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 2.1164\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 2.0891\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 2s 335ms/step - loss: 2.0763\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 2.0427\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 2s 307ms/step - loss: 2.0139\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 2s 290ms/step - loss: 1.9889\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 2s 299ms/step - loss: 1.9611\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 2s 278ms/step - loss: 1.9385\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 2s 308ms/step - loss: 1.9170\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 2s 302ms/step - loss: 1.8980\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 2s 269ms/step - loss: 1.8604\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 2s 271ms/step - loss: 1.8302\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 2s 280ms/step - loss: 1.8124\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 2s 306ms/step - loss: 1.7997\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 2s 343ms/step - loss: 1.7870\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 2s 311ms/step - loss: 1.7494\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 2s 286ms/step - loss: 1.7348\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 2s 304ms/step - loss: 1.7045\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 2s 327ms/step - loss: 1.6941\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 2s 299ms/step - loss: 1.6542\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 2s 289ms/step - loss: 1.6372\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 2s 295ms/step - loss: 1.6396\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 1.5928\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 2s 297ms/step - loss: 1.5844\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 2s 301ms/step - loss: 1.5436\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 2s 302ms/step - loss: 1.5409\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 2s 293ms/step - loss: 1.5221\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 2s 295ms/step - loss: 1.5004\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 2s 298ms/step - loss: 1.4874\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 2s 301ms/step - loss: 1.4599\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 2s 293ms/step - loss: 1.4516\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 2s 312ms/step - loss: 1.4222\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 2s 380ms/step - loss: 1.4061\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 2s 297ms/step - loss: 1.4011\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 2s 310ms/step - loss: 1.3802\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 2s 326ms/step - loss: 1.3684\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 1.3311\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 2s 366ms/step - loss: 1.3183\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 2s 318ms/step - loss: 1.3002\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 2s 317ms/step - loss: 1.3051\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 2s 314ms/step - loss: 1.2746\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 2s 367ms/step - loss: 1.2673\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 3s 492ms/step - loss: 1.2554\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 3s 510ms/step - loss: 1.2168\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 3s 443ms/step - loss: 1.2086\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 2s 360ms/step - loss: 1.1987\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 2s 393ms/step - loss: 1.1714\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 2s 344ms/step - loss: 1.1703\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 2s 302ms/step - loss: 1.1304\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 2s 297ms/step - loss: 1.1343\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 2s 354ms/step - loss: 1.1261\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 2s 325ms/step - loss: 1.1143\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 2s 342ms/step - loss: 1.0982\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 2s 315ms/step - loss: 1.0859\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 2s 329ms/step - loss: 1.0636\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 2s 301ms/step - loss: 1.0481\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 2s 303ms/step - loss: 1.0370\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 2s 344ms/step - loss: 1.0240\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 2s 323ms/step - loss: 1.0038\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 2s 307ms/step - loss: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137b56050>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating history of the model\n",
    "model.fit(predictors,\n",
    "          label, \n",
    "          epochs=100,\n",
    "          batch_size=128, \n",
    "          verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has trained, we can then use this to generate *new text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 15:42:39.677312: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-28 15:42:39.680683: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-28 15:42:39.684715: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "Danish I I Dont Is The\n"
     ]
    }
   ],
   "source": [
    "# print text based on the word/words and what the model has predicted would go along with it\n",
    "print(generate_text(\"danish\", 5, model, max_sequence_len)) # the 5 is the number of words we want to come after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "outpath = os.path.join(\"../model/rnn_model.keras\")\n",
    "tf.keras.saving.save_model(model, outpath, overwrite=True, save_format=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 15:42:53.274212: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-28 15:42:53.279169: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-28 15:42:53.282803: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# importing\n",
    "loaded_model = tf.keras.saving.load_model(\"../model/rnn_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate_text() missing 1 required positional argument: 'max_sequence_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(generate_text(\u001b[39m\"\u001b[39;49m\u001b[39mdanish\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m5\u001b[39;49m, loaded_model))\n",
      "\u001b[0;31mTypeError\u001b[0m: generate_text() missing 1 required positional argument: 'max_sequence_len'"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"danish\", 5, loaded_model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OTHER METHOD\n",
    "\n",
    "import joblib\n",
    "from joblib import dump\n",
    "# saving trained model\n",
    "dump(history, \"../out/rnn_model.joblib\")\n",
    "\n",
    "# load model\n",
    "loaded_model = joblib.load(\"../out/rnn_model.joblib\")\n",
    "print(generate_text(\"danish\", 5, model, max_sequence_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the embedding layer as a trainable parameter, we can instead using a *pretrained word embedding* model like ```word2vec```.\n",
    "\n",
    "In the following examples, we're using [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). These are trained a little differently from ```word2vec``` but they behave in the same way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of already trained word embeddings, which are better than what we ever could train on a model ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         word, coefs \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(maxsplit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m         coefs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfromstring(coefs, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m         embeddings_index[word] \u001b[39m=\u001b[39m coefs\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFound \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m word vectors.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(embeddings_index))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# defining path\n",
    "path_to_glove_file = os.path.join(\"../data/glove/\")\n",
    "\n",
    "# creating pre-trained word embedding index\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some variables that we're going to use later.\n",
    "\n",
    "With hits and misses, we're counting how many words in the corpus vocabulary have a corresponding GloVe embedding; misses are the words which appear in our vocabulary but which do not have a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer - notice that this is different\n",
    "    model.add(Embedding(\n",
    "            total_words,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=input_len)\n",
    "    )\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"china\", 30, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
