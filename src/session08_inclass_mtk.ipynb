{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8 - Language modelling with RNNs (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 15:20:00.424613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# data processing tools\n",
    "import string, os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# keras module for building LSTM \n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow.keras.utils as ku \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# surpress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower() # make all text lowercase, keep strings that arent punctuation\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore') # utf8 encoding makes up for accents\n",
    "    return txt \n",
    "\n",
    "def get_sequence_of_tokens(tokenizer, corpus):\n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    # get the length of the longest sequence\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    # make every sequence the length of the longest on\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                            maxlen=max_sequence_len, \n",
    "                                            padding='pre'))\n",
    "\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, \n",
    "                            num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, \n",
    "                        10, \n",
    "                        input_length=input_len))\n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1)) # remove 10% of the weight when the model is training\n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], \n",
    "                                    maxlen=max_sequence_len-1, \n",
    "                                    padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list),\n",
    "                                            axis=1)   \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"../in/news_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're then going to load the data one at a time and append *only* the headlines to our list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Comments' in filename:\n",
    "        comment_df = pd.read_csv(data_dir + \"/\" + filename)\n",
    "        all_comments.extend(list(comment_df[\"commentBody\"].values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then clean up a little bit and see how many data points we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203199"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments = [c for c in all_comments if c != \"Unknown\"] # remove all unknown headlines\n",
    "len(all_comments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call out ```clean_text()``` function and then inspect the first 10 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i typically strongly dislike articles which bring up such tiny disadvantages to women but this article i liked it mentioned with many female politicians being replaced by male lawmakers who have pushed for legislation to limit womens access to abortion which would normally tick my nonfeminist self off in a regular old article but i feel as though the beating of their drums are beating a new perspective in my heart the bahia group is going against cultural norms by becoming the first allfemale blocoafro in brazil and that problem was certainly not tiny they are drumming for their rights as women they are drumming for their rights of their race this article has influenced me to stand for any matter i want to defend with determination just as these women did in banda dida',\n",
       " 'i went to cuba twice in 19989 to study afrocuban drumming  i am a woman  i didnt experience any prejudicei suppose money silences thatbut we also visited matanzas where we listened to stellar drummers in the home of a woman who was the leader of the group  unusual not unheard of',\n",
       " 'i dont think this is really all that new i was in salvador brasil in august 2013  there was an all female drumming group that performed near the pelourinho that blew the crowd away with their energy and skill  perhaps it wasnt dida that i saw but it was a large organized group of women drummers',\n",
       " 'i was in salvador last month and got to see dida before attending the ballet show in pelourinho they are amazing and im excited to return and go to one of their performances ive always admired female drummers',\n",
       " 'why is it that men take it upon themselves to try to determine where and in what capacity women can exist  listen up men weve had enough of your controlling behavior and were not going to take it any more  times up',\n",
       " 'opa',\n",
       " 'not necessarily the death knell for majorettes',\n",
       " 'bahia is definitely one of my favorite places on earth whats not to love about a city closing down its center on a tuesday night so that we can party with olodum all night long',\n",
       " 'yes women just do it out there is much more freedom than you might expect btw i a white hetero women started playing the drums and riding large motorcycles 40 years ago without any involvement of a man',\n",
       " 'drumming also makes you go deaf early but hey look at that posture and those upper arms more power to you']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_comments]\n",
    "corpus[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "We're then going to tokenize our data, using the ```Tokenizer()``` class from ```TensorFlow```, about which you can read more [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "\n",
    "We then use the ```get_sequence_of_tokens()``` function we defined above, which turns every text into a sequence of tokens based on the vocabulary from the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code creates an index of the words in the corpus\n",
    "tokenizer = Tokenizer()\n",
    "## tokenization\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1 # +1 accounts for words that arent in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 3312],\n",
       " [10, 3312, 2492],\n",
       " [10, 3312, 2492, 3929],\n",
       " [10, 3312, 2492, 3929, 1280],\n",
       " [10, 3312, 2492, 3929, 1280, 81],\n",
       " [10, 3312, 2492, 3929, 1280, 81, 505],\n",
       " [10, 3312, 2492, 3929, 1280, 81, 505, 60],\n",
       " [10, 3312, 2492, 3929, 1280, 81, 505, 60, 147],\n",
       " [10, 3312, 2492, 3929, 1280, 81, 505, 60, 147, 2168],\n",
       " [10, 3312, 2492, 3929, 1280, 81, 505, 60, 147, 2168, 18169]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take input and turn it into numerical output\n",
    "inp_sequences = get_sequence_of_tokens(tokenizer, corpus)\n",
    "inp_sequences[:10]\n",
    "# the length of the input sequences are defined by how the words relate to each other"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to *pad* our input sequences to make them all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding makes different inputs the same length as the longest input by adding 0's where words are missing in inputs\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n",
    "# E.g. with \"My cat\" and \"The big dog\" a 0 would be added in the front like \"0 My cat\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the ```create_model()``` function created above to initialize a model, telling the model the length of sequences and the total size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 23, 10)            112650    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               44400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 11265)             1137765   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 13:28:48.180028: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-22 13:28:48.182795: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-22 13:28:48.185262: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,294,815\n",
      "Trainable params: 1,294,815\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is exactly the same as last week, but instead of document labels, we're fitting the model to predict next word.\n",
    "\n",
    "*NB!* This will take some time to train! It took me 35 minutes on UCloud 32xCPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 13:28:56.409244: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-22 13:28:56.413034: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-22 13:28:56.415629: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-03-22 13:28:58.345896: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-22 13:28:58.350534: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-22 13:28:58.353624: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405/405 [==============================] - 34s 71ms/step - loss: 7.8848\n",
      "Epoch 2/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 7.4818\n",
      "Epoch 3/100\n",
      "405/405 [==============================] - 27s 66ms/step - loss: 7.3958\n",
      "Epoch 4/100\n",
      "405/405 [==============================] - 30s 73ms/step - loss: 7.3057\n",
      "Epoch 5/100\n",
      "405/405 [==============================] - 29s 73ms/step - loss: 7.2025\n",
      "Epoch 6/100\n",
      "405/405 [==============================] - 27s 68ms/step - loss: 7.0930\n",
      "Epoch 7/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 6.9781\n",
      "Epoch 8/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 6.8568\n",
      "Epoch 9/100\n",
      "405/405 [==============================] - 26s 63ms/step - loss: 6.7294\n",
      "Epoch 10/100\n",
      "405/405 [==============================] - 25s 63ms/step - loss: 6.6030\n",
      "Epoch 11/100\n",
      "405/405 [==============================] - 26s 64ms/step - loss: 6.4755\n",
      "Epoch 12/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 6.3505\n",
      "Epoch 13/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 6.2302\n",
      "Epoch 14/100\n",
      "405/405 [==============================] - 24s 59ms/step - loss: 6.1133\n",
      "Epoch 15/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 5.9975\n",
      "Epoch 16/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 5.8849\n",
      "Epoch 17/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 5.7765\n",
      "Epoch 18/100\n",
      "405/405 [==============================] - 25s 60ms/step - loss: 5.6673\n",
      "Epoch 19/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 5.5610\n",
      "Epoch 20/100\n",
      "405/405 [==============================] - 25s 63ms/step - loss: 5.4548\n",
      "Epoch 21/100\n",
      "405/405 [==============================] - 29s 70ms/step - loss: 5.3535\n",
      "Epoch 22/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 5.2511\n",
      "Epoch 23/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 5.1540\n",
      "Epoch 24/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 5.0605\n",
      "Epoch 25/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 4.9667\n",
      "Epoch 26/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 4.8736\n",
      "Epoch 27/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 4.7901\n",
      "Epoch 28/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 4.7045\n",
      "Epoch 29/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 4.6255\n",
      "Epoch 30/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 4.5455\n",
      "Epoch 31/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 4.4724\n",
      "Epoch 32/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 4.4023\n",
      "Epoch 33/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 4.3309\n",
      "Epoch 34/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 4.2626\n",
      "Epoch 35/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 4.2063\n",
      "Epoch 36/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 4.1434\n",
      "Epoch 37/100\n",
      "405/405 [==============================] - 29s 72ms/step - loss: 4.0861\n",
      "Epoch 38/100\n",
      "405/405 [==============================] - 26s 65ms/step - loss: 4.0311\n",
      "Epoch 39/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.9759\n",
      "Epoch 40/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.9269\n",
      "Epoch 41/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.8731\n",
      "Epoch 42/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.8299\n",
      "Epoch 43/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.7836\n",
      "Epoch 44/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.7389\n",
      "Epoch 45/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.6952\n",
      "Epoch 46/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.6576\n",
      "Epoch 47/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.6155\n",
      "Epoch 48/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.5706\n",
      "Epoch 49/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.5343\n",
      "Epoch 50/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.4966\n",
      "Epoch 51/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.4602\n",
      "Epoch 52/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.4311\n",
      "Epoch 53/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.3962\n",
      "Epoch 54/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.3613\n",
      "Epoch 55/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.3283\n",
      "Epoch 56/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.3006\n",
      "Epoch 57/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.2653\n",
      "Epoch 58/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.2389\n",
      "Epoch 59/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 3.2051\n",
      "Epoch 60/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.1808\n",
      "Epoch 61/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 3.1495\n",
      "Epoch 62/100\n",
      "405/405 [==============================] - 27s 68ms/step - loss: 3.1196\n",
      "Epoch 63/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 3.0960\n",
      "Epoch 64/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.0709\n",
      "Epoch 65/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 3.0419\n",
      "Epoch 66/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 3.0150\n",
      "Epoch 67/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.9984\n",
      "Epoch 68/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.9734\n",
      "Epoch 69/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.9449\n",
      "Epoch 70/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.9291\n",
      "Epoch 71/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.9045\n",
      "Epoch 72/100\n",
      "405/405 [==============================] - 24s 60ms/step - loss: 2.8825\n",
      "Epoch 73/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.8626\n",
      "Epoch 74/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.8345\n",
      "Epoch 75/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.8193\n",
      "Epoch 76/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.7974\n",
      "Epoch 77/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 2.7825\n",
      "Epoch 78/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.7607\n",
      "Epoch 79/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.7395\n",
      "Epoch 80/100\n",
      "405/405 [==============================] - 28s 69ms/step - loss: 2.7229\n",
      "Epoch 81/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.7050\n",
      "Epoch 82/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.6840\n",
      "Epoch 83/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.6708\n",
      "Epoch 84/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 2.6497\n",
      "Epoch 85/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.6322\n",
      "Epoch 86/100\n",
      "405/405 [==============================] - 26s 65ms/step - loss: 2.6162\n",
      "Epoch 87/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.6032\n",
      "Epoch 88/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.5876\n",
      "Epoch 89/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.5762\n",
      "Epoch 90/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.5654\n",
      "Epoch 91/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.5441\n",
      "Epoch 92/100\n",
      "405/405 [==============================] - 25s 62ms/step - loss: 2.5304\n",
      "Epoch 93/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.5113\n",
      "Epoch 94/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.5013\n",
      "Epoch 95/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.4891\n",
      "Epoch 96/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.4760\n",
      "Epoch 97/100\n",
      "405/405 [==============================] - 26s 65ms/step - loss: 2.4643\n",
      "Epoch 98/100\n",
      "405/405 [==============================] - 25s 61ms/step - loss: 2.4513\n",
      "Epoch 99/100\n",
      "405/405 [==============================] - 28s 70ms/step - loss: 2.4396\n",
      "Epoch 100/100\n",
      "405/405 [==============================] - 27s 67ms/step - loss: 2.4248\n"
     ]
    }
   ],
   "source": [
    "# creating history of the model\n",
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model has trained, we can then use this to generate *new text*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 14:11:10.505500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-22 14:11:10.507676: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-22 14:11:10.510464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 824ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Danish Inventor Is Found Guilty In\n"
     ]
    }
   ],
   "source": [
    "# print text based on the word/words and what the model has predicted would go along with it\n",
    "print(generate_text(\"danish\", 5, model, max_sequence_len)) # the 5 is the number of words we want to come after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../out/rnn_model.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving\n",
    "import joblib\n",
    "from joblib import dump\n",
    "# saving trained model\n",
    "dump(history, \"../out/rnn_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 14:26:42.620445: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-03-22 14:26:42.623448: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-03-22 14:26:42.625440: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Danish Inventor Is Found Guilty In\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "loaded_model = joblib.load(\"../out/rnn_model.joblib\")\n",
    "print(generate_text(\"danish\", 5, model, max_sequence_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having the embedding layer as a trainable parameter, we can instead using a *pretrained word embedding* model like ```word2vec```.\n",
    "\n",
    "In the following examples, we're using [GloVe embeddings](https://nlp.stanford.edu/projects/glove/). These are trained a little differently from ```word2vec``` but they behave in the same way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of already trained word embeddings, which are better than what we ever could train on a model ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         word, coefs \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(maxsplit\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m         coefs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfromstring(coefs, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m         embeddings_index[word] \u001b[39m=\u001b[39m coefs\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFound \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m word vectors.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(embeddings_index))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# defining path\n",
    "path_to_glove_file = os.path.join(\"../data/glove/\")\n",
    "\n",
    "# creating pre-trained word embedding index\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define some variables that we're going to use later.\n",
    "\n",
    "With hits and misses, we're counting how many words in the corpus vocabulary have a corresponding GloVe embedding; misses are the words which appear in our vocabulary but which do not have a GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = total_words\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer - notice that this is different\n",
    "    model.add(Embedding(\n",
    "            total_words,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "            input_length=input_len)\n",
    "    )\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, \n",
    "                    activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                    optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(predictors, \n",
    "                    label, \n",
    "                    epochs=100,\n",
    "                    batch_size=128, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_text(\"china\", 30, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
